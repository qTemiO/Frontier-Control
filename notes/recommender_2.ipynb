{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries and Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from gensim.models import Word2Vec \n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "from nltk import word_tokenize\n",
    "import warnings;\n",
    "warnings.filterwarnings('ignore')\n",
    "from collections import Counter "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw = pd.read_excel('tnved-CIS-02.xls')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TNVED</th>\n",
       "      <th>Cust_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Unit</th>\n",
       "      <th>FULL_TEXT</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0101</td>\n",
       "      <td>6116.0</td>\n",
       "      <td>Лошади, ослы, мулы и лошаки живые:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ:</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210000</td>\n",
       "      <td>5129.0</td>\n",
       "      <td>––чистопородные племенные животные</td>\n",
       "      <td>шт</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ЧИ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>010129</td>\n",
       "      <td>6730.0</td>\n",
       "      <td>––прочие:</td>\n",
       "      <td>NaN</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101291000</td>\n",
       "      <td>6689.0</td>\n",
       "      <td>–––убойные</td>\n",
       "      <td>шт</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101299000</td>\n",
       "      <td>5596.0</td>\n",
       "      <td>–––прочие</td>\n",
       "      <td>шт</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        TNVED  Cust_ID                                Name Unit  \\\n",
       "0        0101   6116.0  Лошади, ослы, мулы и лошаки живые:  NaN   \n",
       "1  0101210000   5129.0  ––чистопородные племенные животные   шт   \n",
       "2      010129   6730.0                           ––прочие:  NaN   \n",
       "3  0101291000   6689.0                          –––убойные   шт   \n",
       "4  0101299000   5596.0                           –––прочие   шт   \n",
       "\n",
       "                                           FULL_TEXT  \n",
       "0                 ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ:  \n",
       "1   ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ЧИ...  \n",
       "2   ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...  \n",
       "3   ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...  \n",
       "4   ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15927, 5)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The dataset contains 541,909 transactions. That is a pretty good number for us."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Treat Missing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TNVED           0\n",
       "Cust_ID       135\n",
       "Name           31\n",
       "Unit         4375\n",
       "FULL_TEXT      31\n",
       "dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check for missing values\n",
    "df_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "Since we have sufficient data, we will drop all the rows with missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TNVED        0\n",
       "Cust_ID      0\n",
       "Name         0\n",
       "Unit         0\n",
       "FULL_TEXT    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# remove missing values\n",
    "df_raw.dropna(inplace=True)\n",
    "\n",
    "# again check missing values\n",
    "df_raw.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's convert the StockCode to string datatype."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_raw['TNVED']= df_raw['TNVED'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's check out the number of unique customers in our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1996"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "customers = df_raw[\"Cust_ID\"].unique().tolist()\n",
    "len(customers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Jora\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['и',\n",
       " 'в',\n",
       " 'во',\n",
       " 'не',\n",
       " 'что',\n",
       " 'он',\n",
       " 'на',\n",
       " 'я',\n",
       " 'с',\n",
       " 'со',\n",
       " 'как',\n",
       " 'а',\n",
       " 'то',\n",
       " 'все',\n",
       " 'она',\n",
       " 'так',\n",
       " 'его',\n",
       " 'но',\n",
       " 'да',\n",
       " 'ты',\n",
       " 'к',\n",
       " 'у',\n",
       " 'же',\n",
       " 'вы',\n",
       " 'за',\n",
       " 'бы',\n",
       " 'по',\n",
       " 'только',\n",
       " 'ее',\n",
       " 'мне',\n",
       " 'было',\n",
       " 'вот',\n",
       " 'от',\n",
       " 'меня',\n",
       " 'еще',\n",
       " 'нет',\n",
       " 'о',\n",
       " 'из',\n",
       " 'ему',\n",
       " 'теперь',\n",
       " 'когда',\n",
       " 'даже',\n",
       " 'ну',\n",
       " 'вдруг',\n",
       " 'ли',\n",
       " 'если',\n",
       " 'уже',\n",
       " 'или',\n",
       " 'ни',\n",
       " 'быть',\n",
       " 'был',\n",
       " 'него',\n",
       " 'до',\n",
       " 'вас',\n",
       " 'нибудь',\n",
       " 'опять',\n",
       " 'уж',\n",
       " 'вам',\n",
       " 'ведь',\n",
       " 'там',\n",
       " 'потом',\n",
       " 'себя',\n",
       " 'ничего',\n",
       " 'ей',\n",
       " 'может',\n",
       " 'они',\n",
       " 'тут',\n",
       " 'где',\n",
       " 'есть',\n",
       " 'надо',\n",
       " 'ней',\n",
       " 'для',\n",
       " 'мы',\n",
       " 'тебя',\n",
       " 'их',\n",
       " 'чем',\n",
       " 'была',\n",
       " 'сам',\n",
       " 'чтоб',\n",
       " 'без',\n",
       " 'будто',\n",
       " 'чего',\n",
       " 'раз',\n",
       " 'тоже',\n",
       " 'себе',\n",
       " 'под',\n",
       " 'будет',\n",
       " 'ж',\n",
       " 'тогда',\n",
       " 'кто',\n",
       " 'этот',\n",
       " 'того',\n",
       " 'потому',\n",
       " 'этого',\n",
       " 'какой',\n",
       " 'совсем',\n",
       " 'ним',\n",
       " 'здесь',\n",
       " 'этом',\n",
       " 'один',\n",
       " 'почти',\n",
       " 'мой',\n",
       " 'тем',\n",
       " 'чтобы',\n",
       " 'нее',\n",
       " 'сейчас',\n",
       " 'были',\n",
       " 'куда',\n",
       " 'зачем',\n",
       " 'всех',\n",
       " 'никогда',\n",
       " 'можно',\n",
       " 'при',\n",
       " 'наконец',\n",
       " 'два',\n",
       " 'об',\n",
       " 'другой',\n",
       " 'хоть',\n",
       " 'после',\n",
       " 'над',\n",
       " 'больше',\n",
       " 'тот',\n",
       " 'через',\n",
       " 'эти',\n",
       " 'нас',\n",
       " 'про',\n",
       " 'всего',\n",
       " 'них',\n",
       " 'какая',\n",
       " 'много',\n",
       " 'разве',\n",
       " 'три',\n",
       " 'эту',\n",
       " 'моя',\n",
       " 'впрочем',\n",
       " 'хорошо',\n",
       " 'свою',\n",
       " 'этой',\n",
       " 'перед',\n",
       " 'иногда',\n",
       " 'лучше',\n",
       " 'чуть',\n",
       " 'том',\n",
       " 'нельзя',\n",
       " 'такой',\n",
       " 'им',\n",
       " 'более',\n",
       " 'всегда',\n",
       " 'конечно',\n",
       " 'всю',\n",
       " 'между',\n",
       " 'прочие',\n",
       " 'включая']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import string\n",
    "import nltk\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "lst_stopwords = nltk.corpus.stopwords.words('russian')\n",
    "lst_stopwords.append('прочие')\n",
    "lst_stopwords.append('включая')\n",
    "import re\n",
    "lst_stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text, tokenizer, stopwords):\n",
    "    \"\"\"Pre-process text and generate tokens\n",
    "\n",
    "    Args:\n",
    "        text: Text to tokenize.\n",
    "\n",
    "    Returns:\n",
    "        Tokenized text.\n",
    "    \"\"\"\n",
    "    text = str(text).lower()  # Lowercase words\n",
    "    text = re.sub(r\"\\[(.*?)\\]\", \"\", text)  # Remove [+XYZ chars] in content\n",
    "    text = re.sub(r\"\\s+\", \" \", text)  # Remove multiple spaces in content\n",
    "    text = re.sub(r\"\\w+…|…\", \"\", text)  # Remove ellipsis (and last word)\n",
    "    text = re.sub(r\"(?<=\\w)-(?=\\w)\", \" \", text)  # Replace dash between words\n",
    "    text = re.sub(\n",
    "        f\"[{re.escape(string.punctuation)}]\", \"\", text\n",
    "    )  # Remove punctuation\n",
    "\n",
    "    tokens = tokenizer(text)  # Get tokens from text\n",
    "    tokens = [t for t in tokens if not t in lst_stopwords]  # Remove stopwords\n",
    "    tokens = [\"\" if t.isdigit() else t for t in tokens]  # Remove digits\n",
    "    tokens = [t for t in tokens if len(t) > 1]  # Remove short tokens\n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original dataframe: (11454, 7)\n",
      "Pre-processed dataframe: (10686, 2)\n"
     ]
    }
   ],
   "source": [
    "text_columns = ['FULL_TEXT']\n",
    "\n",
    "df = df_raw\n",
    "df[ \"FULL_TEXT\"] = df[ \"FULL_TEXT\"].fillna(\" \")\n",
    "\n",
    "for col in text_columns:\n",
    "    df[col] = df[col].astype(str)\n",
    "\n",
    "# Create text column based on title, description, and content\n",
    "df[\"text\"] = df[text_columns].apply(lambda x: \" | \".join(x), axis=1)\n",
    "df[\"tokens\"] = df[\"text\"].map(lambda x: clean_text(x, word_tokenize, lst_stopwords))\n",
    "\n",
    "#Remove duplicated after preprocessing\n",
    "_, idx = np.unique(df[\"tokens\"], return_index=True)\n",
    "df = df.iloc[idx, :]\n",
    "\n",
    "# Remove empty values\n",
    "df = df.loc[df.tokens.map(lambda x: len(x) > 0), [\"text\", \"tokens\"]]\n",
    "\n",
    "print(f\"Original dataframe: {df_raw.shape}\")\n",
    "print(f\"Pre-processed dataframe: {df.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TNVED</th>\n",
       "      <th>Cust_ID</th>\n",
       "      <th>Name</th>\n",
       "      <th>Unit</th>\n",
       "      <th>FULL_TEXT</th>\n",
       "      <th>text</th>\n",
       "      <th>tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0101210000</td>\n",
       "      <td>5129.0</td>\n",
       "      <td>––чистопородные племенные животные</td>\n",
       "      <td>шт</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ЧИ...</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ЧИ...</td>\n",
       "      <td>[лошади, ослы, мулы, лошаки, живые, лошади, чи...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0101291000</td>\n",
       "      <td>6689.0</td>\n",
       "      <td>–––убойные</td>\n",
       "      <td>шт</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...</td>\n",
       "      <td>[лошади, ослы, мулы, лошаки, живые, лошади, уб...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0101299000</td>\n",
       "      <td>5596.0</td>\n",
       "      <td>–––прочие</td>\n",
       "      <td>шт</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...</td>\n",
       "      <td>[лошади, ослы, мулы, лошаки, живые, лошади]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0101300000</td>\n",
       "      <td>5117.0</td>\n",
       "      <td>–ослы</td>\n",
       "      <td>шт</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ОСЛЫ</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ОСЛЫ</td>\n",
       "      <td>[лошади, ослы, мулы, лошаки, живые, ослы]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0101900000</td>\n",
       "      <td>5686.0</td>\n",
       "      <td>–прочие</td>\n",
       "      <td>шт</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ПРОЧИЕ</td>\n",
       "      <td>ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ПРОЧИЕ</td>\n",
       "      <td>[лошади, ослы, мулы, лошаки, живые]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15785</th>\n",
       "      <td>9506620000</td>\n",
       "      <td>5694.0</td>\n",
       "      <td>––мячи надувные</td>\n",
       "      <td>шт</td>\n",
       "      <td>ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...</td>\n",
       "      <td>ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...</td>\n",
       "      <td>[инвентарь, оборудование, занятий, общей, физк...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15787</th>\n",
       "      <td>9506691000</td>\n",
       "      <td>6802.0</td>\n",
       "      <td>–––мячи для крикета и поло</td>\n",
       "      <td>шт</td>\n",
       "      <td>ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...</td>\n",
       "      <td>ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...</td>\n",
       "      <td>[инвентарь, оборудование, занятий, общей, физк...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15788</th>\n",
       "      <td>9506699000</td>\n",
       "      <td>5291.0</td>\n",
       "      <td>–––прочие</td>\n",
       "      <td>шт</td>\n",
       "      <td>ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...</td>\n",
       "      <td>ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...</td>\n",
       "      <td>[инвентарь, оборудование, занятий, общей, физк...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15790</th>\n",
       "      <td>9506701000</td>\n",
       "      <td>5769.0</td>\n",
       "      <td>––ледовые коньки</td>\n",
       "      <td>пар</td>\n",
       "      <td>ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...</td>\n",
       "      <td>ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...</td>\n",
       "      <td>[инвентарь, оборудование, занятий, общей, физк...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15791</th>\n",
       "      <td>9506703000</td>\n",
       "      <td>5572.0</td>\n",
       "      <td>––роликовые коньки</td>\n",
       "      <td>пар</td>\n",
       "      <td>ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...</td>\n",
       "      <td>ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...</td>\n",
       "      <td>[инвентарь, оборудование, занятий, общей, физк...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>11454 rows × 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            TNVED  Cust_ID                                Name Unit  \\\n",
       "1      0101210000   5129.0  ––чистопородные племенные животные   шт   \n",
       "3      0101291000   6689.0                          –––убойные   шт   \n",
       "4      0101299000   5596.0                           –––прочие   шт   \n",
       "5      0101300000   5117.0                               –ослы   шт   \n",
       "6      0101900000   5686.0                             –прочие   шт   \n",
       "...           ...      ...                                 ...  ...   \n",
       "15785  9506620000   5694.0                     ––мячи надувные   шт   \n",
       "15787  9506691000   6802.0          –––мячи для крикета и поло   шт   \n",
       "15788  9506699000   5291.0                           –––прочие   шт   \n",
       "15790  9506701000   5769.0                    ––ледовые коньки  пар   \n",
       "15791  9506703000   5572.0                  ––роликовые коньки  пар   \n",
       "\n",
       "                                               FULL_TEXT  \\\n",
       "1       ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ЧИ...   \n",
       "3       ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...   \n",
       "4       ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...   \n",
       "5                ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ОСЛЫ   \n",
       "6              ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ПРОЧИЕ   \n",
       "...                                                  ...   \n",
       "15785   ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...   \n",
       "15787   ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...   \n",
       "15788   ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...   \n",
       "15790   ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...   \n",
       "15791   ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...   \n",
       "\n",
       "                                                    text  \\\n",
       "1       ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ЧИ...   \n",
       "3       ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...   \n",
       "4       ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ПР...   \n",
       "5                ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ОСЛЫ   \n",
       "6              ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ПРОЧИЕ   \n",
       "...                                                  ...   \n",
       "15785   ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...   \n",
       "15787   ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...   \n",
       "15788   ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...   \n",
       "15790   ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...   \n",
       "15791   ИНВЕНТАРЬ И ОБОРУДОВАНИЕ ДЛЯ ЗАНЯТИЙ ОБЩЕЙ ФИ...   \n",
       "\n",
       "                                                  tokens  \n",
       "1      [лошади, ослы, мулы, лошаки, живые, лошади, чи...  \n",
       "3      [лошади, ослы, мулы, лошаки, живые, лошади, уб...  \n",
       "4            [лошади, ослы, мулы, лошаки, живые, лошади]  \n",
       "5              [лошади, ослы, мулы, лошаки, живые, ослы]  \n",
       "6                    [лошади, ослы, мулы, лошаки, живые]  \n",
       "...                                                  ...  \n",
       "15785  [инвентарь, оборудование, занятий, общей, физк...  \n",
       "15787  [инвентарь, оборудование, занятий, общей, физк...  \n",
       "15788  [инвентарь, оборудование, занятий, общей, физк...  \n",
       "15790  [инвентарь, оборудование, занятий, общей, физк...  \n",
       "15791  [инвентарь, оборудование, занятий, общей, физк...  \n",
       "\n",
       "[11454 rows x 7 columns]"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are 4,372 customers in our dataset. For each of these customers we will extract their buying history. In other words, we can have 4,372 sequences of purchases."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is a good practice to set aside a small part of the dataset for validation purpose. Therefore, I will use data of 90% of the customers to create word2vec embeddings. Let's split the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# shuffle customer ID's\n",
    "random.shuffle(customers)\n",
    "\n",
    "# extract 90% of customer ID's\n",
    "customers_train = [customers[i] for i in range(round(0.9*len(customers)))]\n",
    "\n",
    "# split data into train and validation set\n",
    "train_df = df_raw[df_raw['Cust_ID'].isin(customers_train)]\n",
    "validation_df = df_raw[~df_raw['Cust_ID'].isin(customers_train)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's create sequences of purchases made by the customers in the dataset for both the train and validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████| 1796/1796 [00:00<00:00, 5189.58it/s]\n"
     ]
    }
   ],
   "source": [
    "# list to capture purchase history of the customers\n",
    "purchases_train = []\n",
    "\n",
    "# populate the list with the product codes\n",
    "for i in tqdm(customers_train):\n",
    "    temp = train_df[train_df[\"Cust_ID\"] == i][\"TNVED\"].tolist()\n",
    "    purchases_train.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████| 200/200 [00:00<00:00, 5262.05it/s]\n"
     ]
    }
   ],
   "source": [
    "# list to capture purchase history of the customers\n",
    "purchases_val = []\n",
    "\n",
    "# populate the list with the product codes\n",
    "for i in tqdm(validation_df['Cust_ID'].unique()):\n",
    "    temp = validation_df[validation_df[\"Cust_ID\"] == i][\"TNVED\"].tolist()\n",
    "    purchases_val.append(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs = df_raw[\"tokens\"].values\n",
    "tokenized_docs = df_raw[\"tokens\"].values\n",
    "vocab = Counter()\n",
    "for token in tokenized_docs:\n",
    "    vocab.update(token)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build word2vec Embeddings for Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train word2vec model\n",
    "model = Word2Vec(window = 10, sg = 1, hs = 0,\n",
    "                 negative = 10, # for negative sampling\n",
    "                 alpha=0.03, min_alpha=0.0007,\n",
    "                 seed = 14)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.build_vocab(tokenized_docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6878222, 7517850)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.train(tokenized_docs, total_examples=model.corpus_count, epochs=30, report_delay=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save word2vec model\n",
    "model.save(\"word2vec_2.model\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we do not plan to train the model any further, we are calling init_sims(), which will make the model much more memory-efficient."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.init_sims(replace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=5634, size=100, alpha=0.03)\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we will extract the vectors of all the words in our vocabulary and store it in one place for easy access."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(5634, 100)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# extract all vectors\n",
    "X = model[model.wv.vocab]\n",
    "\n",
    "X.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('грудинки', 0.9217213988304138),\n",
       " ('стрики', 0.9154728651046753),\n",
       " ('корейки', 0.9049151539802551),\n",
       " ('окорока', 0.8635772466659546),\n",
       " ('края', 0.8194581270217896),\n",
       " ('лопатки', 0.8155902624130249),\n",
       " ('баранина', 0.8007454872131348),\n",
       " ('козлятина', 0.7877663373947144),\n",
       " ('пищевую', 0.7829128503799438),\n",
       " ('пищевая', 0.765634298324585)]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.wv.similar_by_vector(\"свинина\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start Recommending Products"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "products = train_df[[\"TNVED\", \"FULL_TEXT\"]]\n",
    "\n",
    "# remove duplicates\n",
    "products.drop_duplicates(inplace=True, subset='TNVED', keep=\"last\")\n",
    "\n",
    "# create product-ID and product-description dictionary\n",
    "products_dict = products.groupby('TNVED')['FULL_TEXT'].apply(list).to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[' ЛОШАДИ, ОСЛЫ, МУЛЫ И ЛОШАКИ ЖИВЫЕ: ЛОШАДИ: ЧИСТОПОРОДНЫЕ ПЛЕМЕННЫЕ ЖИВОТНЫЕ']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test the dictionary\n",
    "products_dict['0101210000']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "def similar_products(v, n = 6):\n",
    "    \n",
    "    # extract most similar products for the input vector\n",
    "    ms = model.similar_by_vector(v, topn= n+1)[1:]\n",
    "    \n",
    "    # extract name and similarity score of the similar products\n",
    "    new_ms = []\n",
    "    for j in ms:\n",
    "        pair = (products_dict[j[0]][0], j[1])\n",
    "        new_ms.append(pair)\n",
    "        \n",
    "    return new_ms        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"word 'ЛОШАДИ' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-40-a2a60c341322>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msimilar_products\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilar_by_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'ЛОШАДИ'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\utils.py\u001b[0m in \u001b[0;36mnew_func1\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m   1445\u001b[0m                     \u001b[0mstacklevel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m2\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1446\u001b[0m                 )\n\u001b[1;32m-> 1447\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1448\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1449\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mnew_func1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\base_any2vec.py\u001b[0m in \u001b[0;36msimilar_by_word\u001b[1;34m(self, word, topn, restrict_vocab)\u001b[0m\n\u001b[0;32m   1423\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1424\u001b[0m         \"\"\"\n\u001b[1;32m-> 1425\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwv\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilar_by_word\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1426\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1427\u001b[0m     \u001b[1;33m@\u001b[0m\u001b[0mdeprecated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"Method will be removed in 4.0.0, use self.wv.similar_by_vector() instead\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36msimilar_by_word\u001b[1;34m(self, word, topn, restrict_vocab)\u001b[0m\n\u001b[0;32m    594\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    595\u001b[0m         \"\"\"\n\u001b[1;32m--> 596\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtopn\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    597\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    598\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0msimilar_by_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mvector\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrestrict_vocab\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, restrict_vocab, indexer)\u001b[0m\n\u001b[0;32m    551\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0muse_norm\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m                     \u001b[0mall_words\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvocab\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\anaconda3\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'ЛОШАДИ' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "similar_products(model.similar_by_word('ЛОШАДИ'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
